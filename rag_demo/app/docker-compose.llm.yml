services:
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-gemma
    gpus: all
    ports:
      - "8000:8000"
    volumes:
      - "C:/llm/gguf:/models:ro"
      # D드라이브 로그 폴더는 일단 마운트만 해둠(지금은 사용 안 함)
      - "D:/llm/logs/llama:/logs"
    command: >
      -m /models/gemma-2-9b-it-Q4_K_M-fp16.gguf
      --alias gemma-2-9b-it
      -c 16384
      -b 512
      -ub 64
      -ngl 999
      --host 0.0.0.0
      --port 8000
      --chat-template gemma
      -v
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
    restart: unless-stopped
