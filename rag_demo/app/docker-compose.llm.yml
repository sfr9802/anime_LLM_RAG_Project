services:
  llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llm
    command:
      - -m
      - /models/gemma-2-9b-it-Q4_K_M-fp16.gguf
      - --alias
      - gemma-2-9b-it
      - -c
      - "8192"
      - -ngl
      - "999"
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      # - --api
      # - --chat-template
      - gemma
    ports:
      - "8000:8000"
    volumes:
      - "C:/llm/gguf:/models:ro"
    gpus: all
    restart: unless-stopped
