---
annotations_creators:
  - no-annotation
language:
  - ko
license:
  - cc-by-nc-sa-2.0
multilinguality: monolingual
size_categories:
  - 1K<n<10K
source_datasets:
  - external
task_categories:
  - question-answering
  - text-retrieval
pretty_name: NamuWiki Anime RAG
tags:
  - anime
  - namuwiki
  - korean
  - rag
  - llm
  - retrieval-augmented-generation
---

# NamuWiki Anime RAG Dataset

A Korean-language dataset of anime-related documents collected and refined from NamuWiki. Designed for **Retrieval-Augmented Generation (RAG)**, semantic search, and LLM-based QA systems.

---

## ğŸ“¦ Dataset Overview

- **Total documents (raw)**: ~2.7k (varies slightly by rebuild)  
- **Format**: JSONL  
- **Language**: Korean  
- **Domain**: Japanese animation (TV series, OVAs, movies)  
- **Source**: NamuWiki (CC BY-NC-SA 2.0 KR)

> This repository ships two formats at the **repo root** (no subdirectories): the original page-level crawl (**v1**) and a processed, per-work format (**v2**) tailored for RAG.

---

## ğŸ“ Versions (root files)

- **v1 (raw)** â€” page-level crawl  
  **File**: `namuwiki_anime_raw.jsonl`  
  (fields such as `title`, `url`, `parent`, `metadata.seed_title`, and either `chunks` **or** `content`)

- **v2 (processed, per-seed RAG)** â€” **one record per work (seed)** with normalized sections and a **character list**  
  **File**: `out_with_chars.jsonl`

**When to use which**
- Use **v2** for RAG/QA pipelines (section routing, character-aware retrieval, deduplicated chunks).  
- Use **v1** if you plan to run your own preprocessing or alternative normalization.

---

## ğŸ“„ v1 (Raw) Document Format

Each line in the raw JSONL may look like:

```json
{
  "title": "Demon Slayer",
  "url": "https://namu.wiki/w/ê·€ë©¸ì˜%20ì¹¼ë‚ ",
  "parent": null,
  "metadata": {
    "seed_title": "Demon Slayer",
    "depth": 0,
    "fetched_at": "2025-08-08T11:23:11.123456"
  },
  "chunks": [
    "Demon Slayer is a Japanese anime adapted from the manga by Koyoharu Gotouge.",
    "It aired in 2019, produced by Ufotable."
  ],
  "content": "...\n(full text when chunks are absent)\n..."
}
```

---

## ğŸ§± v2 (Processed, Per-Seed RAG Format)

**One JSON object = one work (seed).** Sections are normalized and merged; **character pages** (where `parent âˆˆ {"ë“±ì¥ì¸ë¬¼","ìºë¦­í„°","ì¸ë¬¼"}`) are aggregated into `sections.ë“±ì¥ì¸ë¬¼.list`.

### v2 Schema

```json
{
  "seed": "ê·€ë©¸ì˜ ì¹¼ë‚ ",
  "title": "ê·€ë©¸ì˜ ì¹¼ë‚ ",
  "sections": {
    "ë³¸ë¬¸": {
      "text": "ì‘í’ˆ ê°œìš”...\\n\\nì„¸ê³„ê´€ ì„¤ëª…...",
      "chunks": ["ì‘í’ˆ ê°œìš”...", "ì„¸ê³„ê´€ ì„¤ëª…..."],
      "urls": ["https://namu.wiki/w/ê·€ë©¸ì˜ ì¹¼ë‚ "]
    },
    "ì¤„ê±°ë¦¬": { "text": "...", "chunks": ["..."], "urls": ["..."] },
    "ì„¤ì •":   { "text": "...", "chunks": ["..."], "urls": ["..."] },
    "ë“±ì¥ì¸ë¬¼": {
      "text": "ë“±ì¥ì¸ë¬¼ ì´ë¡ (ìˆì„ ê²½ìš°)",
      "chunks": ["..."],
      "urls": ["..."],
      "list": [
        { "name": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ", "desc": "ì£¼ì¸ê³µ. ë¬¼ì˜ í˜¸í¡...", "url": "https://namu.wiki/w/ì¹´ë§ˆë„ íƒ„ì§€ë¡œ" },
        { "name": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”", "desc": "ì—¬ë™ìƒ. ê·€í™”...",      "url": "https://namu.wiki/w/ì¹´ë§ˆë„ ë„¤ì¦ˆì½”" }
      ]
    }
  },
  "section_order": ["ë³¸ë¬¸","ì¤„ê±°ë¦¬","ì„¤ì •","ë“±ì¥ì¸ë¬¼"],
  "meta": { "seed_title": "ê·€ë©¸ì˜ ì¹¼ë‚ " },
  "doc_id": "9a2b1c0d4e5f6a7b8c9d01ab"
}
```

#### Normalization Rules
- Section aliases â†’ canonical keys: `ê°œìš”/ì‹œë†‰ì‹œìŠ¤ â†’ ì¤„ê±°ë¦¬`, `ì„¸ê³„ê´€ â†’ ì„¤ì •`, `ìºë¦­í„°/ì¸ë¬¼ â†’ ë“±ì¥ì¸ë¬¼`
- `chunks` are deduplicated by content hash; `text = "\\n\\n".join(chunks)`  
- `section_order` lists only existing sections

#### Character List Construction
- From raw pages where `parent` marks a character page  
  (`ë“±ì¥ì¸ë¬¼`, `ìºë¦­í„°`, or `ì¸ë¬¼`), grouped by `metadata.seed_title`
- `name = title`, `desc = cleaned content`, `url = page URL`
- Extremely short bios are filtered with a configurable threshold (`--min_char_desc`, default 20 chars)

---

## ğŸ§¹ Noise Filtering (per line)

Lines removed include (non-exhaustive):

- Copyright / CAPTCHA: `Â©`, `All Rights Reserved`, `reCAPTCHA`, `hCaptcha`
- Social/platform tails: `YouTube`, `TikTok`, `X(íŠ¸ìœ„í„°)`, OTT/platform names
- News/press headers: `ê¸°ì`, `ë³´ë„ìë£Œ`, `ì—°í•©ë‰´ìŠ¤`, `[ë‹¨ë…]`, `ì‚¬ì§„=`, `ì¶œì²˜=`
- Game PR buzz: `CBT/OBT/ì‚¬ì „ë“±ë¡/ì¶œì‹œ/ë¡ ì¹­`, vendor names (e.g., `ê·¸ë¼ë¹„í‹°`, `ë„¥ìŠ¨`, ...)
- Relative-time headers: `3ë¶„ ì „`, `2ì‹œê°„ ì „`, `5ì¼ ì „`  
  *(story text like `25ë…„ ì „` is preserved)*
- Hashtag-only short lines, excessive Latin-only noise

---

## ğŸ“ˆ v2 Quick Stats

- **Records**: ~2,700 works  
- **Have `ë“±ì¥ì¸ë¬¼`**: ~500 works  
- **Avg characters per work**: ~7â€“8 (max around 10)

> Numbers can vary slightly by rebuild and filter settings.

---

## ğŸ”§ Loading Examples

### Load v2 (processed)

```python
from datasets import load_dataset

ds = load_dataset("ArinNya/namuwiki_anime",
                  data_files="out_with_chars.jsonl",
                  split="train")

r = ds[0]
print(r["seed"], list((r["sections"] or {}).keys()))
chars = (r["sections"] or {}).get("ë“±ì¥ì¸ë¬¼", {}).get("list", [])
print(len(chars), chars[:3])
```

### Build a FAISS index (toy)

```python
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

ds = load_dataset("ArinNya/namuwiki_anime",
                  data_files="out_with_chars.jsonl",
                  split="train")

model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
texts, metas = [], []

for ex in ds:
    seed = ex["seed"]
    sections = ex.get("sections") or {}
    # index section-level chunks
    for sec_name, sec in sections.items():
        for c in sec.get("chunks", []):
            texts.append(c)
            metas.append({"seed": seed, "section": sec_name})
    # index characters as well
    for c in (sections.get("ë“±ì¥ì¸ë¬¼", {}) or {}).get("list", []):
        texts.append(f"[{seed}] {c.get('name')}: {c.get('desc')}")
        metas.append({"seed": seed, "section": "ë“±ì¥ì¸ë¬¼", "name": c.get("name")})

emb = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
index = faiss.IndexFlatIP(emb.shape[1])
faiss.normalize_L2(emb)
index.add(emb)
```

---

## ğŸ” Cleaning Criteria (v1 & v2)

**Removed**
- Advertisement-only pages, CAPTCHA blocks, evident news/press/PR headers
- Lines with only hashtags or excessive Latin-only noise

**Preserved**
- Short but relevant lines for core sections (characters/settings/plot)
- Minor tail noise replaced with empty strings when safer

---

## ğŸ’¡ Use Cases

- Korean anime domain **RAG** (per-seed context + section routing)
- Character-centric QA / entity retrieval
- Dense retrieval experiments on community-edited encyclopedic text

---

## âš ï¸ License

- Source: NamuWiki  
- Derived under **CC BY-NC-SA 2.0 KR**  
- **Non-commercial use only**, ShareAlike  
- License field in metadata: `cc-by-nc-sa-2.0`

---

## ğŸ§‘â€ğŸ’» Credits

- Author: [@ArinNya](https://github.com/sfr9802)  
- Blog: https://arin-nya.tistory.com/

---

## ğŸ—“ï¸ Why start from 2006?

*The Melancholy of Haruhi Suzumiya* (2006) significantly shaped modern character-driven anime aesthetics and light-novel culture; this dataset focuses on the contemporary â€œmoe/bishoujoâ€ era as a practical boundary for downstream tasks.

---

# ë‚˜ë¬´ìœ„í‚¤ ì• ë‹ˆë©”ì´ì…˜ RAG ë°ì´í„°ì…‹

í•œêµ­ì–´ ìœ„í‚¤ì¸ **ë‚˜ë¬´ìœ„í‚¤**ì—ì„œ ì• ë‹ˆë©”ì´ì…˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³ , **RAG/QA ì‹¤í—˜**ì— ì í•©í•˜ë„ë¡ ì •ì œí–ˆìŠµë‹ˆë‹¤.

## ğŸ“ ë²„ì „ (ë£¨íŠ¸ íŒŒì¼)

- **v1 (raw)** â€” í˜ì´ì§€ ë‹¨ìœ„ í¬ë¡¤ ê²°ê³¼  
  **íŒŒì¼**: `namuwiki_anime_raw.jsonl`

- **v2 (processed, ì‘í’ˆ ë‹¨ìœ„ RAG)** â€” **ì‘í’ˆë‹¹ 1ë ˆì½”ë“œ**, í‘œì¤€ ì„¹ì…˜/ë“±ì¥ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸ í¬í•¨  
  **íŒŒì¼**: `out_with_chars.jsonl`

## ğŸ§± v2 ìŠ¤í‚¤ë§ˆ

```json
{
  "seed": "ì‘í’ˆëª…",
  "title": "ëŒ€í‘œ ì œëª©",
  "sections": {
    "ë³¸ë¬¸": { "text": "...", "chunks": ["..."], "urls": ["..."] },
    "ì¤„ê±°ë¦¬": { "text": "...", "chunks": ["..."], "urls": ["..."] },
    "ì„¤ì •":   { "text": "...", "chunks": ["..."], "urls": ["..."] },
    "ë“±ì¥ì¸ë¬¼": {
      "text": "...(ì´ë¡ )",
      "chunks": ["..."],
      "urls": ["..."],
      "list": [
        { "name": "ì´ë¦„", "desc": "ì„¤ëª…", "url": "..." }
      ]
    }
  },
  "section_order": ["ë³¸ë¬¸","ì¤„ê±°ë¦¬","ì„¤ì •","ë“±ì¥ì¸ë¬¼"],
  "meta": { "seed_title": "..." },
  "doc_id": "md5(seed)[:24]"
}
```

**ì •ê·œí™” ê·œì¹™**
- `ê°œìš”/ì‹œë†‰ì‹œìŠ¤ â†’ ì¤„ê±°ë¦¬`, `ì„¸ê³„ê´€ â†’ ì„¤ì •`, `ìºë¦­í„°/ì¸ë¬¼ â†’ ë“±ì¥ì¸ë¬¼`
- `chunks` ì¤‘ë³µ ì œê±° í›„ `text` ê²°í•©, `section_order`ëŠ” ì¡´ì¬ ì„¹ì…˜ë§Œ ë‚˜ì—´
- ë“±ì¥ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸ëŠ” `parent âˆˆ {ë“±ì¥ì¸ë¬¼, ìºë¦­í„°, ì¸ë¬¼}`ì¸ **ê°œë³„ ì¸ë¬¼ í˜ì´ì§€**ë¥¼ `metadata.seed_title` ê¸°ì¤€ìœ¼ë¡œ ë¬¶ì–´ì„œ ìƒì„±

**ë…¸ì´ì¦ˆ í•„í„° (ë¼ì¸ ë‹¨ìœ„)**
- ì €ì‘ê¶Œ/ë¦¬ìº¡ì± /í”Œë«í¼ ê¼¬ë¦¬, ë‰´ìŠ¤Â·ë³´ë„ í—¤ë”, ê²Œì„ í™ë³´(CBT/ì¶œì‹œ ë“±), ìƒëŒ€ì‹œê°„ í—¤ë”(ë¶„/ì‹œê°„/ì¼ ì „), í•´ì‹œíƒœê·¸-only, ë¼í‹´ ê³¼ë‹¤

## ğŸ“ˆ v2 ëŒ€ëµ í†µê³„

- ë ˆì½”ë“œ: ì•½ 2.7k  
- `ë“±ì¥ì¸ë¬¼` ë³´ìœ : ì•½ 500  
- ì‘í’ˆë‹¹ í‰ê·  ìºë¦­í„° ìˆ˜: 7â€“8ëª…

## ğŸ’¡ í™œìš© ì˜ˆì‹œ

- ì‘í’ˆ ë‹¨ìœ„ ì»¨í…ìŠ¤íŠ¸ + ì„¹ì…˜ ë¼ìš°íŒ… ê¸°ë°˜ **RAG**
- ìºë¦­í„° ì¤‘ì‹¬ QA / ì—”í‹°í‹° ê²€ìƒ‰
- ì»¤ë®¤ë‹ˆí‹° í¸ì§‘ ë°±ê³¼ ë¬¸ì„œì— ëŒ€í•œ ë°€ì§‘ ê²€ìƒ‰ ì‹¤í—˜

## ğŸ” ì •ì œ ê¸°ì¤€ (v1 & v2)

**ì œê±°**
- ê´‘ê³ /ìº¡ì°¨/ë‰´ìŠ¤Â·ë³´ë„/ê²Œì„ PR í—¤ë”
- í•´ì‹œíƒœê·¸-only, ë¼í‹´ ê³¼ë‹¤ ë…¸ì´ì¦ˆ

**ë³´ì¡´**
- ë“±ì¥ì¸ë¬¼/ì„¤ì •/ì¤„ê±°ë¦¬ ë“± í•µì‹¬ ì„¹ì…˜ ê´€ë ¨ ì§§ì€ ë¬¸ì¥
- ì¼ë¶€ ê¼¬ë¦¬ ë…¸ì´ì¦ˆëŠ” ì•ˆì „í•œ ë²”ìœ„ì—ì„œ ì¹˜í™˜/ì‚­ì œ

## âš ï¸ ë¼ì´ì„ ìŠ¤

- ì›ë³¸: ë‚˜ë¬´ìœ„í‚¤ / **CC BY-NC-SA 2.0 KR**  
- **ë¹„ìƒì—…ì  ì‚¬ìš©**, ë³€ê²½ ì‹œ ë™ì¼ ë¼ì´ì„ ìŠ¤ ê³µìœ   
- License: `cc-by-nc-sa-2.0`

## ğŸ§‘â€ğŸ’» ì œì‘ ë° ê¸°ì—¬

- ì‘ì„±ì: [@ArinNya](https://github.com/sfr9802)  
- ë¸”ë¡œê·¸: https://arin-nya.tistory.com/

## ğŸ“… ì™œ 2006ë…„ë¶€í„° ìˆ˜ì§‘í–ˆë‚˜ìš”?

**ã€ŠìŠ¤ì¦ˆë¯¸ì•¼ í•˜ë£¨íˆì˜ ìš°ìš¸ã€‹(2006)**ì„ ê¸°ì ìœ¼ë¡œ ìºë¦­í„° ì¤‘ì‹¬ ë¯¸í•™ê³¼ ë¼ì´íŠ¸ë…¸ë²¨ ë¬¸í™”ê°€ í¬ê²Œ í™•ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ë°ì´í„°ì…‹ì€ ì´ ì‹œê¸°ë¥¼ í˜„ëŒ€ â€œëª¨ì—/ë¯¸ì†Œë…€â€ ì‹œëŒ€ì˜ ì‹¤ìš©ì  ê²½ê³„ë¡œ ë³´ê³  ìˆ˜ì§‘Â·ì •ì œë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
